{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 110] Connection timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    977\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1455\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1455\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1456\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m month \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      2\u001b[0m dataset_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.csv.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_chunks \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(dataset_url,compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m df_chunks\u001b[39m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/common.py:716\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    713\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    715\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    717\u001b[0m     path_or_buf,\n\u001b[1;32m    718\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    719\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    720\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    721\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    725\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/common.py:368\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    367\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[0;32m--> 368\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[1;32m    369\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    371\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/common.py:270\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 110] Connection timed out>"
     ]
    }
   ],
   "source": [
    "month = 1\n",
    "dataset_url = f\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{month:02}.csv.gz\"\n",
    "df_chunks = pd.read_csv(dataset_url,compression='gzip')\n",
    "df_chunks.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_chunks\u001b[39m.\u001b[39mshape()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "df_chunks.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>2019-01-01 02:51:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>2019-01-01 00:54:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>2019-01-01 00:54:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:19:00</td>\n",
       "      <td>2019-01-01 00:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:27:00</td>\n",
       "      <td>2019-01-01 00:37:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>B02869</td>\n",
       "      <td>2019-01-01 01:46:01</td>\n",
       "      <td>2019-01-01 01:54:10</td>\n",
       "      <td>29.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>B02869</td>\n",
       "      <td>2019-01-01 01:57:38</td>\n",
       "      <td>2019-01-01 02:02:52</td>\n",
       "      <td>210.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>B02869</td>\n",
       "      <td>2019-01-01 01:10:03</td>\n",
       "      <td>2019-01-01 01:31:16</td>\n",
       "      <td>38.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>B02869</td>\n",
       "      <td>2019-01-01 01:36:45</td>\n",
       "      <td>2019-01-01 01:52:01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>B02869</td>\n",
       "      <td>2019-01-01 01:54:55</td>\n",
       "      <td>2019-01-01 02:18:06</td>\n",
       "      <td>223.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B02869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0                   B00001  2019-01-01 00:30:00  2019-01-01 02:51:55   \n",
       "1                   B00001  2019-01-01 00:45:00  2019-01-01 00:54:49   \n",
       "2                   B00001  2019-01-01 00:15:00  2019-01-01 00:54:52   \n",
       "3                   B00008  2019-01-01 00:19:00  2019-01-01 00:39:00   \n",
       "4                   B00008  2019-01-01 00:27:00  2019-01-01 00:37:00   \n",
       "...                    ...                  ...                  ...   \n",
       "99995               B02869  2019-01-01 01:46:01  2019-01-01 01:54:10   \n",
       "99996               B02869  2019-01-01 01:57:38  2019-01-01 02:02:52   \n",
       "99997               B02869  2019-01-01 01:10:03  2019-01-01 01:31:16   \n",
       "99998               B02869  2019-01-01 01:36:45  2019-01-01 01:52:01   \n",
       "99999               B02869  2019-01-01 01:54:55  2019-01-01 02:18:06   \n",
       "\n",
       "       PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0               NaN           NaN      NaN                 B00001  \n",
       "1               NaN           NaN      NaN                 B00001  \n",
       "2               NaN           NaN      NaN                 B00001  \n",
       "3               NaN           NaN      NaN                 B00008  \n",
       "4               NaN           NaN      NaN                 B00008  \n",
       "...             ...           ...      ...                    ...  \n",
       "99995          29.0         149.0      NaN                 B02869  \n",
       "99996         210.0         150.0      NaN                 B02869  \n",
       "99997          38.0          73.0      NaN                 B02869  \n",
       "99998           9.0         223.0      NaN                 B02869  \n",
       "99999         223.0          48.0      NaN                 B02869  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = next(df_chunks)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'data/local.parquet'\n",
    "df.pickup_datetime = pd.to_datetime(df.pickup_datetime)\n",
    "df.dropOff_datetime = pd.to_datetime(df.dropOff_datetime)\n",
    "df.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted chunk 1\n",
      "Inserted chunk 2\n",
      "Inserted chunk 3\n",
      "Inserted chunk 4\n",
      "Inserted chunk 5\n",
      "Inserted chunk 6\n",
      "Inserted chunk 7\n",
      "Inserted chunk 8\n",
      "Inserted chunk 9\n",
      "Inserted chunk 10\n",
      "Inserted chunk 11\n",
      "Inserted chunk 12\n",
      "Inserted chunk 13\n",
      "Inserted chunk 14\n",
      "Inserted chunk 15\n",
      "Inserted chunk 16\n",
      "Inserted chunk 17\n",
      "Inserted chunk 18\n",
      "Inserted chunk 19\n",
      "Inserted chunk 20\n",
      "Inserted chunk 21\n",
      "Inserted chunk 22\n",
      "Inserted chunk 23\n",
      "Inserted chunk 24\n",
      "Inserted chunk 25\n",
      "Inserted chunk 26\n",
      "Inserted chunk 27\n",
      "Inserted chunk 28\n",
      "Inserted chunk 29\n",
      "Inserted chunk 30\n",
      "Inserted chunk 31\n",
      "Inserted chunk 32\n",
      "Inserted chunk 33\n",
      "Inserted chunk 34\n",
      "Inserted chunk 35\n",
      "Inserted chunk 36\n",
      "Inserted chunk 37\n",
      "Inserted chunk 38\n",
      "Inserted chunk 39\n",
      "Inserted chunk 40\n",
      "Inserted chunk 41\n",
      "Inserted chunk 42\n",
      "Inserted chunk 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     chunk\u001b[39m.\u001b[39mdropOff_datetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(chunk\u001b[39m.\u001b[39mdropOff_datetime)\n\u001b[1;32m      5\u001b[0m     combined \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([existing, chunk], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m     combined\u001b[39m.\u001b[39;49mto_parquet(output_path, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInserted chunk \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mChunks inserted successfully\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2890\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2891\u001b[0m     path,\n\u001b[1;32m   2892\u001b[0m     engine,\n\u001b[1;32m   2893\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2894\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2895\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2896\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2898\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    190\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pyarrow/parquet/core.py:3106\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   3084\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3085\u001b[0m             where, table\u001b[39m.\u001b[39mschema,\n\u001b[1;32m   3086\u001b[0m             filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3104\u001b[0m             store_schema\u001b[39m=\u001b[39mstore_schema,\n\u001b[1;32m   3105\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m-> 3106\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_table(table, row_group_size\u001b[39m=\u001b[39;49mrow_group_size)\n\u001b[1;32m   3107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   3108\u001b[0m     \u001b[39mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pyarrow/parquet/core.py:1092\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mTable schema does not match schema used to create file: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1088\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtable:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{!s}\u001b[39;00m\u001b[39m vs. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mfile:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{!s}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m   1089\u001b[0m            \u001b[39m.\u001b[39mformat(table\u001b[39m.\u001b[39mschema, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema))\n\u001b[1;32m   1090\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 1092\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwrite_table(table, row_group_size\u001b[39m=\u001b[39;49mrow_group_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(df_chunks, start=1):\n",
    "    existing = pd.read_parquet(output_path)\n",
    "    chunk.pickup_datetime = pd.to_datetime(chunk.pickup_datetime)\n",
    "    chunk.dropOff_datetime = pd.to_datetime(chunk.dropOff_datetime)\n",
    "    combined = pd.concat([existing, chunk], ignore_index=True)\n",
    "    combined.to_parquet(output_path, index=False)\n",
    "    print(f'Inserted chunk {i}')\n",
    "print('Chunks inserted successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m    df\u001b[39m.\u001b[39mpickup_datetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df\u001b[39m.\u001b[39mpickup_datetime)\n\u001b[1;32m      5\u001b[0m    df\u001b[39m.\u001b[39mdropOff_datetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df\u001b[39m.\u001b[39mdropOff_datetime)\n\u001b[0;32m----> 6\u001b[0m    df\u001b[39m.\u001b[39;49mto_parquet(output_path, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m    \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInserted another chunk\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2890\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2891\u001b[0m     path,\n\u001b[1;32m   2892\u001b[0m     engine,\n\u001b[1;32m   2893\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2894\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2895\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2896\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2898\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    190\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pyarrow/parquet/core.py:3084\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3082\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   3083\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3084\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3085\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   3086\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   3087\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   3088\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m   3089\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   3090\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   3091\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[1;32m   3092\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[1;32m   3093\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[1;32m   3094\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3095\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[1;32m   3096\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   3097\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   3098\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   3099\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   3100\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   3101\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   3102\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   3103\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   3104\u001b[0m             store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[1;32m   3105\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m   3106\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   3107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pyarrow/parquet/core.py:1001\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmetadata_collector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1000\u001b[0m engine_version \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mV2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1001\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m _parquet\u001b[39m.\u001b[39;49mParquetWriter(\n\u001b[1;32m   1002\u001b[0m     sink, schema,\n\u001b[1;32m   1003\u001b[0m     version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   1004\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   1005\u001b[0m     use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   1006\u001b[0m     write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   1007\u001b[0m     use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_deprecated_int96_timestamps,\n\u001b[1;32m   1008\u001b[0m     compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   1009\u001b[0m     use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   1010\u001b[0m     column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   1011\u001b[0m     writer_engine_version\u001b[39m=\u001b[39;49mengine_version,\n\u001b[1;32m   1012\u001b[0m     data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   1013\u001b[0m     use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   1014\u001b[0m     encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   1015\u001b[0m     write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   1016\u001b[0m     dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   1017\u001b[0m     store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[1;32m   1018\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m   1019\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_open \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/coding/DE_learning/DataEngineering/data_talks_camp/homework/3/flow/lib/python3.10/site-packages/pyarrow/_parquet.pyx:1694\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "       df = next(df_chunks)\n",
    "       df.pickup_datetime = pd.to_datetime(df.pickup_datetime)\n",
    "       df.dropOff_datetime = pd.to_datetime(df.dropOff_datetime)\n",
    "       df.to_parquet(output_path, mode='append', index=False)\n",
    "       print('Inserted another chunk')\n",
    "    except StopIteration:\n",
    "        print('Finished inserting data. Thank you')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fhv_tripdata_2019-01.csv\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>2019-01-01 02:51:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>2019-01-01 00:54:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>2019-01-01 00:54:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:19:00</td>\n",
       "      <td>2019-01-01 00:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:27:00</td>\n",
       "      <td>2019-01-01 00:37:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>B00248</td>\n",
       "      <td>2019-01-01 00:11:09</td>\n",
       "      <td>2019-01-01 00:20:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>B00248</td>\n",
       "      <td>2019-01-01 00:52:34</td>\n",
       "      <td>2019-01-01 01:01:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>B00248</td>\n",
       "      <td>2019-01-01 00:17:44</td>\n",
       "      <td>2019-01-01 00:24:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B00248</td>\n",
       "      <td>2019-01-01 00:00:10</td>\n",
       "      <td>2019-01-01 00:33:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>B00248</td>\n",
       "      <td>2019-01-01 00:06:00</td>\n",
       "      <td>2019-01-01 00:36:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0                B00001  2019-01-01 00:30:00  2019-01-01 02:51:55   \n",
       "1                B00001  2019-01-01 00:45:00  2019-01-01 00:54:49   \n",
       "2                B00001  2019-01-01 00:15:00  2019-01-01 00:54:52   \n",
       "3                B00008  2019-01-01 00:19:00  2019-01-01 00:39:00   \n",
       "4                B00008  2019-01-01 00:27:00  2019-01-01 00:37:00   \n",
       "..                  ...                  ...                  ...   \n",
       "95               B00248  2019-01-01 00:11:09  2019-01-01 00:20:59   \n",
       "96               B00248  2019-01-01 00:52:34  2019-01-01 01:01:06   \n",
       "97               B00248  2019-01-01 00:17:44  2019-01-01 00:24:03   \n",
       "98               B00248  2019-01-01 00:00:10  2019-01-01 00:33:26   \n",
       "99               B00248  2019-01-01 00:06:00  2019-01-01 00:36:23   \n",
       "\n",
       "    PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0            NaN           NaN      NaN                 B00001  \n",
       "1            NaN           NaN      NaN                 B00001  \n",
       "2            NaN           NaN      NaN                 B00001  \n",
       "3            NaN           NaN      NaN                 B00008  \n",
       "4            NaN           NaN      NaN                 B00008  \n",
       "..           ...           ...      ...                    ...  \n",
       "95           NaN         265.0      NaN                 B00248  \n",
       "96           NaN         265.0      NaN                 B00248  \n",
       "97           NaN         265.0      NaN                 B00248  \n",
       "98           NaN         265.0      NaN                 B00248  \n",
       "99           NaN         265.0      NaN                 B00248  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"fhv_trips_01\" (\n",
      "\"dispatching_base_num\" TEXT,\n",
      "  \"pickup_datetime\" TEXT,\n",
      "  \"dropOff_datetime\" TEXT,\n",
      "  \"PUlocationID\" REAL,\n",
      "  \"DOlocationID\" REAL,\n",
      "  \"SR_Flag\" REAL,\n",
      "  \"Affiliated_base_number\" TEXT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name=\"fhv_trips_01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pickup_datetime = pd.to_datetime(df.pickup_datetime)\n",
    "df.dropOff_datetime = pd.to_datetime(df.dropOff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"fhv_trips_01\" (\n",
      "\"dispatching_base_num\" TEXT,\n",
      "  \"pickup_datetime\" TIMESTAMP,\n",
      "  \"dropOff_datetime\" TIMESTAMP,\n",
      "  \"PUlocationID\" REAL,\n",
      "  \"DOlocationID\" REAL,\n",
      "  \"SR_Flag\" REAL,\n",
      "  \"Affiliated_base_number\" TEXT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name=\"fhv_trips_01\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
