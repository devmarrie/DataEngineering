{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec34d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4228a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdfe91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ade1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9db00c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/25 15:59:38 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 15:59:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/25 15:59:38 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master marrie:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to marrie/192.168.43.137:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:298)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: marrie/192.168.43.137:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 15:59:58 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master marrie:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to marrie/192.168.43.137:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:298)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: marrie/192.168.43.137:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 16:00:18 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master marrie:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to marrie/192.168.43.137:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:298)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: marrie/192.168.43.137:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 16:00:38 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/03/25 16:00:38 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "24/03/25 16:00:38 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "24/03/25 16:00:38 ERROR AsyncEventQueue: Listener AppStatusListener threw an exception\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:192)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/03/25 16:00:38 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:678)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:678)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\\\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://marrie:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[1;32m    200\u001b[0m         sparkHome,\n\u001b[1;32m    201\u001b[0m         pyFiles,\n\u001b[1;32m    202\u001b[0m         environment,\n\u001b[1;32m    203\u001b[0m         batchSize,\n\u001b[1;32m    204\u001b[0m         serializer,\n\u001b[1;32m    205\u001b[0m         conf,\n\u001b[1;32m    206\u001b[0m         jsc,\n\u001b[1;32m    207\u001b[0m         profiler_cls,\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1586\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:678)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://marrie:7077\")\\\n",
    "        .appName('test')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31db393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://marrie:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://marrie:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8bd64c8750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d223468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e16ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-18 14:54:39|  2020-01-18 15:16:34|                 N|         1|          66|         161|              1|         6.95|       23.0|  0.0|    0.5|      5.31|         0.0|     null|                  0.3|       31.86|           1|        1|                2.75|\n",
      "|       2| 2020-01-21 17:55:07|  2020-01-21 18:10:20|                 N|         1|          74|          42|              1|         1.61|       11.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|        12.8|           2|        1|                 0.0|\n",
      "|       2| 2020-01-24 08:15:41|  2020-01-24 08:29:22|                 N|         1|          82|         173|              1|         2.05|       11.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        11.8|           2|        1|                 0.0|\n",
      "|    null| 2020-01-29 14:18:00|  2020-01-29 14:37:00|              null|      null|         205|          95|           null|         5.32|      29.56| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       32.61|        null|     null|                null|\n",
      "|       2| 2020-01-05 21:54:45|  2020-01-05 22:05:14|                 N|         1|          65|          49|              1|         1.94|        9.0|  0.5|    0.5|      2.06|         0.0|     null|                  0.3|       12.36|           1|        1|                 0.0|\n",
      "|    null| 2020-01-07 05:18:00|  2020-01-07 05:44:00|              null|      null|         136|         107|           null|        12.49|      32.77|  0.0|    0.0|       0.0|         0.0|     null|                  0.3|       33.07|        null|     null|                null|\n",
      "|       2| 2020-01-11 14:06:26|  2020-01-11 14:16:31|                 N|         1|          31|         241|              1|         2.08|        9.5|  0.0|    0.5|      3.09|         0.0|     null|                  0.3|       13.39|           1|        1|                 0.0|\n",
      "|    null| 2020-01-22 12:42:00|  2020-01-22 12:55:00|              null|      null|         155|          39|           null|         2.99|      23.79| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       26.84|        null|     null|                null|\n",
      "|       2| 2020-01-10 20:32:24|  2020-01-10 20:48:05|                 N|         1|          74|         166|              5|         2.31|       11.5|  0.5|    0.5|       0.1|         0.0|     null|                  0.3|        12.9|           1|        1|                 0.0|\n",
      "|       1| 2020-01-21 16:09:22|  2020-01-21 16:30:17|                 N|         1|         244|         142|              1|          5.8|       20.5| 3.75|    0.5|       0.0|         0.0|     null|                  0.3|       25.05|           2|        1|                2.75|\n",
      "|       2| 2020-01-17 10:56:55|  2020-01-17 11:04:27|                 N|         1|           7|         223|              1|         1.62|        7.5|  0.0|    0.5|      1.66|         0.0|     null|                  0.3|        9.96|           1|        1|                 0.0|\n",
      "|       2| 2020-01-16 07:34:58|  2020-01-16 07:51:29|                 N|         1|          74|         243|              6|         4.99|       18.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        18.8|           2|        1|                 0.0|\n",
      "|    null| 2020-01-03 14:37:00|  2020-01-03 14:57:00|              null|      null|          61|          52|           null|         2.92|      14.38|  0.0|    0.0|       0.0|         0.0|     null|                  0.3|       14.68|        null|     null|                null|\n",
      "|       2| 2020-01-19 02:36:01|  2020-01-19 02:45:58|                 N|         5|          65|          49|              2|         1.79|       35.0|  0.0|    0.0|     10.59|         0.0|     null|                  0.3|       45.89|           1|        2|                 0.0|\n",
      "|       2| 2020-01-07 15:35:53|  2020-01-07 15:59:53|                 N|         1|          82|         162|              1|         5.07|       19.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|       22.55|           2|        1|                2.75|\n",
      "|       2| 2020-01-16 19:19:56|  2020-01-16 19:24:55|                 N|         1|          75|         236|              1|         0.97|        5.5|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|       10.05|           2|        1|                2.75|\n",
      "|    null| 2020-01-08 12:02:00|  2020-01-08 12:20:00|              null|      null|         127|          42|           null|         4.96|      21.01| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       24.06|        null|     null|                null|\n",
      "|       2| 2020-01-29 18:21:27|  2020-01-29 18:22:10|                 N|         1|          14|          14|              1|         0.32|        3.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|         4.8|           2|        1|                 0.0|\n",
      "|       2| 2020-01-31 09:28:46|  2020-01-31 09:37:24|                 N|         1|         134|         130|              1|         1.49|        8.5|  0.0|    0.5|       2.0|         0.0|     null|                  0.3|        11.3|           1|        1|                 0.0|\n",
      "|       2| 2020-01-26 17:52:06|  2020-01-26 18:18:05|                 N|         1|         133|          22|              5|         3.94|       18.0|  0.0|    0.5|       2.0|         0.0|     null|                  0.3|       22.75|           1|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f41b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17805f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1456195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3d1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorID',\n",
       " 'congestion_surcharge',\n",
       " 'dropoff_datetime',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'improvement_surcharge',\n",
       " 'mta_tax',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'pickup_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'trip_distance'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_green.columns) & set(df_yellow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c029f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the collumns  preserving the order\n",
    "common_cols = []\n",
    "\n",
    "yellow_col = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_col:\n",
    "        common_cols.append(col)\n",
    "# print the common cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10756407",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = [\n",
    "    'VendorID',\n",
    "    'pickup_datetime',\n",
    "    'dropoff_datetime',\n",
    "    'store_and_fwd_flag',\n",
    "    'RatecodeID',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'payment_type',\n",
    "    'congestion_surcharge'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3fa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|congestion_surcharge|\n",
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+\n",
      "|       2|2020-01-18 14:54:39|2020-01-18 15:16:34|                 N|         1|          66|         161|              1|         6.95|       23.0|  0.0|    0.5|      5.31|         0.0|                  0.3|       31.86|           1|                2.75|\n",
      "|       2|2020-01-21 17:55:07|2020-01-21 18:10:20|                 N|         1|          74|          42|              1|         1.61|       11.0|  1.0|    0.5|       0.0|         0.0|                  0.3|        12.8|           2|                 0.0|\n",
      "|       2|2020-01-24 08:15:41|2020-01-24 08:29:22|                 N|         1|          82|         173|              1|         2.05|       11.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.8|           2|                 0.0|\n",
      "|    null|2020-01-29 14:18:00|2020-01-29 14:37:00|              null|      null|         205|          95|           null|         5.32|      29.56| 2.75|    0.0|       0.0|         0.0|                  0.3|       32.61|        null|                null|\n",
      "|       2|2020-01-05 21:54:45|2020-01-05 22:05:14|                 N|         1|          65|          49|              1|         1.94|        9.0|  0.5|    0.5|      2.06|         0.0|                  0.3|       12.36|           1|                 0.0|\n",
      "|    null|2020-01-07 05:18:00|2020-01-07 05:44:00|              null|      null|         136|         107|           null|        12.49|      32.77|  0.0|    0.0|       0.0|         0.0|                  0.3|       33.07|        null|                null|\n",
      "|       2|2020-01-11 14:06:26|2020-01-11 14:16:31|                 N|         1|          31|         241|              1|         2.08|        9.5|  0.0|    0.5|      3.09|         0.0|                  0.3|       13.39|           1|                 0.0|\n",
      "|    null|2020-01-22 12:42:00|2020-01-22 12:55:00|              null|      null|         155|          39|           null|         2.99|      23.79| 2.75|    0.0|       0.0|         0.0|                  0.3|       26.84|        null|                null|\n",
      "|       2|2020-01-10 20:32:24|2020-01-10 20:48:05|                 N|         1|          74|         166|              5|         2.31|       11.5|  0.5|    0.5|       0.1|         0.0|                  0.3|        12.9|           1|                 0.0|\n",
      "|       1|2020-01-21 16:09:22|2020-01-21 16:30:17|                 N|         1|         244|         142|              1|          5.8|       20.5| 3.75|    0.5|       0.0|         0.0|                  0.3|       25.05|           2|                2.75|\n",
      "|       2|2020-01-17 10:56:55|2020-01-17 11:04:27|                 N|         1|           7|         223|              1|         1.62|        7.5|  0.0|    0.5|      1.66|         0.0|                  0.3|        9.96|           1|                 0.0|\n",
      "|       2|2020-01-16 07:34:58|2020-01-16 07:51:29|                 N|         1|          74|         243|              6|         4.99|       18.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        18.8|           2|                 0.0|\n",
      "|    null|2020-01-03 14:37:00|2020-01-03 14:57:00|              null|      null|          61|          52|           null|         2.92|      14.38|  0.0|    0.0|       0.0|         0.0|                  0.3|       14.68|        null|                null|\n",
      "|       2|2020-01-19 02:36:01|2020-01-19 02:45:58|                 N|         5|          65|          49|              2|         1.79|       35.0|  0.0|    0.0|     10.59|         0.0|                  0.3|       45.89|           1|                 0.0|\n",
      "|       2|2020-01-07 15:35:53|2020-01-07 15:59:53|                 N|         1|          82|         162|              1|         5.07|       19.0|  0.0|    0.5|       0.0|         0.0|                  0.3|       22.55|           2|                2.75|\n",
      "|       2|2020-01-16 19:19:56|2020-01-16 19:24:55|                 N|         1|          75|         236|              1|         0.97|        5.5|  1.0|    0.5|       0.0|         0.0|                  0.3|       10.05|           2|                2.75|\n",
      "|    null|2020-01-08 12:02:00|2020-01-08 12:20:00|              null|      null|         127|          42|           null|         4.96|      21.01| 2.75|    0.0|       0.0|         0.0|                  0.3|       24.06|        null|                null|\n",
      "|       2|2020-01-29 18:21:27|2020-01-29 18:22:10|                 N|         1|          14|          14|              1|         0.32|        3.0|  1.0|    0.5|       0.0|         0.0|                  0.3|         4.8|           2|                 0.0|\n",
      "|       2|2020-01-31 09:28:46|2020-01-31 09:37:24|                 N|         1|         134|         130|              1|         1.49|        8.5|  0.0|    0.5|       2.0|         0.0|                  0.3|        11.3|           1|                 0.0|\n",
      "|       2|2020-01-26 17:52:06|2020-01-26 18:18:05|                 N|         1|         133|          22|              5|         3.94|       18.0|  0.0|    0.5|       2.0|         0.0|                  0.3|       22.75|           1|                 0.0|\n",
      "+--------+-------------------+-------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------------------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can now select te columns\n",
    "df_green.select(common_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804bc51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_green' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the lit function to create a new column literal\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_green_serv \u001b[38;5;241m=\u001b[39m df_green \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(common_cols) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice_type\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_green' is not defined"
     ]
    }
   ],
   "source": [
    "# use the lit function to create a new column literal\n",
    "df_green_serv = df_green \\\n",
    "    .select(common_cols) \\\n",
    "    .withColumn('service_type', F.lit('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_serv = df_yellow \\\n",
    "    .select(common_cols) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes into one \n",
    "df_trips_data = df_green_serv.unionAll(df_yellow_serv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now we can count the whole data per service type\n",
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389205c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_trips_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Regiter the dataframe as a temporary table \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# so that it can be able to be called by spark as a table in a select statement\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# registerTempTable() depricated\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_trips_data\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrips_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_trips_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Regiter the dataframe as a temporary table \n",
    "# so that it can be able to be called by spark as a table in a select statement\n",
    "# registerTempTable() depricated\n",
    "df_trips_data.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now run sql queries\n",
    "spark.sql(\"\"\"\n",
    "SELECT service_type,\n",
    "       count(1)\n",
    "FROM trips_data\n",
    "GROUP BY service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df471cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue per month\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afca4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "|revenue_zone|      revenue_month|service_type|revenue_monthly_fare|revenue_monthly_extra|revenue_monthly_mta_tax|revenue_monthly_tip_amount|revenue_monthly_tolls_amount|revenue_monthly_improvement_surcharge|revenue_monthly_total_amount|revenue_monthly_congestion_surcharge|avg_montly_passenger_count|avg_montly_trip_distance|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "|         164|2020-01-01 00:00:00|       green|  1139.5300000000002|                68.75|                    2.5|                       0.0|          116.28000000000003|                   10.800000000000004|                     1337.86|                                 0.0|                       1.0|       8.912222222222223|\n",
      "|         162|2020-01-01 00:00:00|       green|             2281.46|               145.75|                    9.0|                     11.52|          137.44000000000003|                   20.700000000000028|           2605.870000000001|                                 0.0|                       1.0|       9.524927536231884|\n",
      "|         116|2019-12-01 00:00:00|       green|               21.59|                  1.0|                    1.0|                      4.95|                         0.0|                   0.8999999999999999|          29.439999999999998|                                 0.0|                       1.0|      0.6366666666666666|\n",
      "|         265|2020-02-01 00:00:00|       green|  12431.599999999993|               359.09|                  118.0|                    353.03|          461.36000000000024|                    98.69999999999942|          13827.279999999955|                                2.75|        1.5217391304347827|       8.645274725274719|\n",
      "|         152|2020-03-01 00:00:00|       green|  19619.170000000002|               631.75|                  771.0|                   1514.21|          243.70000000000016|                   508.50000000001353|          23938.429999999476|                              786.25|        1.2320544554455446|      2.6666533180778034|\n",
      "|         130|2019-12-01 00:00:00|       green|                34.0|                  2.0|                    2.0|                      2.56|                         0.0|                                  1.2|          41.760000000000005|                                 0.0|                       1.0|                  2.0225|\n",
      "|         116|2020-03-01 00:00:00|       green|   43750.34999999999|               1099.0|                 1577.0|         3493.650000000001|           727.3300000000003|                   1053.8999999999317|           53259.28000000178|                              1589.0|        1.1985988425220835|      2.9585321864595007|\n",
      "|         118|2020-03-01 00:00:00|       green|               218.3|                  1.5|                    1.5|                     12.22|                         0.0|                                  1.8|                      235.32|                                 0.0|                      2.25|       7.208333333333333|\n",
      "|         132|2020-10-01 00:00:00|       green|               341.5|                  1.5|                    2.5|        24.060000000000002|                       12.24|                                  1.8|                      386.35|                                2.75|        1.1666666666666667|      16.474999999999998|\n",
      "|         138|2020-02-01 00:00:00|       green|              982.34|                 6.75|                   17.5|                    143.64|                       18.36|                   15.300000000000015|           1189.389999999999|                                 5.5|                       1.3|       2.475686274509804|\n",
      "|         241|2020-03-01 00:00:00|       green|             8555.63|                 56.5|                   69.5|         38.02000000000001|          382.96000000000026|                   104.09999999999937|           9266.210000000001|                                2.75|        1.0608108108108107|       6.826011080332408|\n",
      "|          72|2020-03-01 00:00:00|       green|  13489.420000000016|                 75.0|                   82.0|                     74.45|           282.2700000000001|                   201.30000000000192|          14354.689999999951|                                 5.5|        1.1338028169014085|      4.8159411764705835|\n",
      "|          95|2020-10-01 00:00:00|       green|   40654.45999999997|                772.5|                 1009.5|         4648.049999999998|           859.0900000000005|                    786.2999999999721|           49006.85000000095|                              294.25|         1.293168880455408|        3.60683520599251|\n",
      "|         155|2020-01-01 00:00:00|       green|  25022.440000000082|              1798.25|                  212.0|                      9.01|          429.11000000000035|                   255.00000000000395|          27730.509999999977|                                2.75|        1.2418300653594772|       5.510752475247514|\n",
      "|          98|2020-01-01 00:00:00|       green|   5543.489999999995|                344.5|                   38.0|                      6.46|          109.64000000000003|                    60.89999999999978|           6110.440000000002|                                 5.5|         1.173913043478261|       5.632169811320751|\n",
      "|         115|2020-01-01 00:00:00|       green|  1960.1399999999996|                 29.0|                   14.0|                       0.0|           434.5200000000002|                   11.100000000000005|          2448.7599999999998|                                 0.0|                      1.52|      14.539487179487178|\n",
      "|         194|2020-01-01 00:00:00|       green|  1151.5000000000002|                 41.5|                   11.5|                     11.14|           201.9600000000001|                    9.900000000000002|                      1442.4|                                11.0|        1.2857142857142858|       7.282142857142857|\n",
      "|         243|2019-12-01 00:00:00|       green|               13.99|                  0.0|                    0.0|                       1.0|                         0.0|                                  0.3|                       15.29|                                 0.0|                       1.0|                     0.0|\n",
      "|         116|2020-02-01 00:00:00|       green|   77899.19999999984|              3196.25|                 2770.5|         6215.790000000005|          1176.9699999999975|                   1809.2999999998171|             95437.610000007|                             2694.25|         1.196932299012694|       2.854020003226329|\n",
      "|          92|2020-03-01 00:00:00|       green|  31863.549999999992|               577.75|                  700.0|        1657.3199999999993|           516.3900000000003|                    552.3000000000076|          35991.259999999704|                                60.5|        1.1630506245890861|      4.4981574718800195|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/03 18:13:03 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/10/03 18:13:03 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:218)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:923)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba99265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('data/reports/revenue/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spark submit on the script\n",
    "URL=\"spark://marrie:7077\"\n",
    "\n",
    "spark-submit \\\n",
    "    --master=\"${URL}\" \\\n",
    "    06_spark_sql.py \\\n",
    "        --input_green=data/pq/green/2021/*/ \\\n",
    "        --input_yellow=data/pq/yellow/2021/*/ \\\n",
    "        --output=data/report-2021\n",
    "\n",
    "# to google\n",
    "--input_green=gs://dtc_data_lake_arcane-grin-394118/pq/green/2021/*/ \\\n",
    "--input_yellow=gs://dtc_data_lake_arcane-grin-394118/pq/yellow/2021/*/ \\\n",
    "--output=gs://dtc_data_lake_arcane-grin-394118/report-2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
