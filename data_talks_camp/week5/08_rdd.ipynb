{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa384572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddfe4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e138d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0eea785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aece2cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/05 08:32:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName('test_rdd') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67b4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcb014",
   "metadata": {},
   "source": [
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857bcd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 18, 14, 54, 39), lpep_dropoff_datetime=datetime.datetime(2020, 1, 18, 15, 16, 34), store_and_fwd_flag='N', RatecodeID=1, PULocationID=66, DOLocationID=161, passenger_count=1, trip_distance=6.95, fare_amount=23.0, extra=0.0, mta_tax=0.5, tip_amount=5.31, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=31.86, payment_type=1, trip_type=1, congestion_surcharge=2.75),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 21, 17, 55, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 21, 18, 10, 20), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=42, passenger_count=1, trip_distance=1.61, fare_amount=11.0, extra=1.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.8, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 24, 8, 15, 41), lpep_dropoff_datetime=datetime.datetime(2020, 1, 24, 8, 29, 22), store_and_fwd_flag='N', RatecodeID=1, PULocationID=82, DOLocationID=173, passenger_count=1, trip_distance=2.05, fare_amount=11.0, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=11.8, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 14, 18), lpep_dropoff_datetime=datetime.datetime(2020, 1, 29, 14, 37), store_and_fwd_flag=None, RatecodeID=None, PULocationID=205, DOLocationID=95, passenger_count=None, trip_distance=5.32, fare_amount=29.56, extra=2.75, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=32.61, payment_type=None, trip_type=None, congestion_surcharge=None),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 21, 54, 45), lpep_dropoff_datetime=datetime.datetime(2020, 1, 5, 22, 5, 14), store_and_fwd_flag='N', RatecodeID=1, PULocationID=65, DOLocationID=49, passenger_count=1, trip_distance=1.94, fare_amount=9.0, extra=0.5, mta_tax=0.5, tip_amount=2.06, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.36, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 5, 18), lpep_dropoff_datetime=datetime.datetime(2020, 1, 7, 5, 44), store_and_fwd_flag=None, RatecodeID=None, PULocationID=136, DOLocationID=107, passenger_count=None, trip_distance=12.49, fare_amount=32.77, extra=0.0, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=33.07, payment_type=None, trip_type=None, congestion_surcharge=None),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 11, 14, 6, 26), lpep_dropoff_datetime=datetime.datetime(2020, 1, 11, 14, 16, 31), store_and_fwd_flag='N', RatecodeID=1, PULocationID=31, DOLocationID=241, passenger_count=1, trip_distance=2.08, fare_amount=9.5, extra=0.0, mta_tax=0.5, tip_amount=3.09, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=13.39, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 22, 12, 42), lpep_dropoff_datetime=datetime.datetime(2020, 1, 22, 12, 55), store_and_fwd_flag=None, RatecodeID=None, PULocationID=155, DOLocationID=39, passenger_count=None, trip_distance=2.99, fare_amount=23.79, extra=2.75, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=26.84, payment_type=None, trip_type=None, congestion_surcharge=None),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 10, 20, 32, 24), lpep_dropoff_datetime=datetime.datetime(2020, 1, 10, 20, 48, 5), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=166, passenger_count=5, trip_distance=2.31, fare_amount=11.5, extra=0.5, mta_tax=0.5, tip_amount=0.1, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.9, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 21, 16, 9, 22), lpep_dropoff_datetime=datetime.datetime(2020, 1, 21, 16, 30, 17), store_and_fwd_flag='N', RatecodeID=1, PULocationID=244, DOLocationID=142, passenger_count=1, trip_distance=5.8, fare_amount=20.5, extra=3.75, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=25.05, payment_type=2, trip_type=1, congestion_surcharge=2.75)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebdeab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "      .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "      .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60d48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fa30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2021, month=1, day=1)\n",
    "\n",
    "# def filter_outliers(row):\n",
    "#     return row['lpep_pickup_datetime'] >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617e387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = rdd.take(10)\n",
    "# row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6099a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the where clause\n",
    "df_result = rdd \\\n",
    "    .filter(lambda row: row['lpep_pickup_datetime'] >= start) \\\n",
    "    .map(lambda row:(\n",
    "    (row['lpep_pickup_datetime'].replace(minute=0, second=0, microsecond=0), row['PULocationID']),\n",
    "    (row['total_amount'], 1)\n",
    "    ))\n",
    "    .reduceByKey(lambda left_value, right_value: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ea2f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the groupby\n",
    "mapped_rdd = filtered_rdd.map(lambda row:(\n",
    "    (row['lpep_pickup_datetime'].replace(minute=0, second=0, microsecond=0), row['PULocationID']),\n",
    "    (row['total_amount'], 1)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ef476be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2020-01-18 14:54:39|  2020-01-18 15:16:34|                 N|         1|          66|         161|              1|         6.95|       23.0|  0.0|    0.5|      5.31|         0.0|     null|                  0.3|       31.86|           1|        1|                2.75|\n",
      "|       2| 2020-01-21 17:55:07|  2020-01-21 18:10:20|                 N|         1|          74|          42|              1|         1.61|       11.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|        12.8|           2|        1|                 0.0|\n",
      "|       2| 2020-01-24 08:15:41|  2020-01-24 08:29:22|                 N|         1|          82|         173|              1|         2.05|       11.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        11.8|           2|        1|                 0.0|\n",
      "|    null| 2020-01-29 14:18:00|  2020-01-29 14:37:00|              null|      null|         205|          95|           null|         5.32|      29.56| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       32.61|        null|     null|                null|\n",
      "|       2| 2020-01-05 21:54:45|  2020-01-05 22:05:14|                 N|         1|          65|          49|              1|         1.94|        9.0|  0.5|    0.5|      2.06|         0.0|     null|                  0.3|       12.36|           1|        1|                 0.0|\n",
      "|    null| 2020-01-07 05:18:00|  2020-01-07 05:44:00|              null|      null|         136|         107|           null|        12.49|      32.77|  0.0|    0.0|       0.0|         0.0|     null|                  0.3|       33.07|        null|     null|                null|\n",
      "|       2| 2020-01-11 14:06:26|  2020-01-11 14:16:31|                 N|         1|          31|         241|              1|         2.08|        9.5|  0.0|    0.5|      3.09|         0.0|     null|                  0.3|       13.39|           1|        1|                 0.0|\n",
      "|    null| 2020-01-22 12:42:00|  2020-01-22 12:55:00|              null|      null|         155|          39|           null|         2.99|      23.79| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       26.84|        null|     null|                null|\n",
      "|       2| 2020-01-10 20:32:24|  2020-01-10 20:48:05|                 N|         1|          74|         166|              5|         2.31|       11.5|  0.5|    0.5|       0.1|         0.0|     null|                  0.3|        12.9|           1|        1|                 0.0|\n",
      "|       1| 2020-01-21 16:09:22|  2020-01-21 16:30:17|                 N|         1|         244|         142|              1|          5.8|       20.5| 3.75|    0.5|       0.0|         0.0|     null|                  0.3|       25.05|           2|        1|                2.75|\n",
      "|       2| 2020-01-17 10:56:55|  2020-01-17 11:04:27|                 N|         1|           7|         223|              1|         1.62|        7.5|  0.0|    0.5|      1.66|         0.0|     null|                  0.3|        9.96|           1|        1|                 0.0|\n",
      "|       2| 2020-01-16 07:34:58|  2020-01-16 07:51:29|                 N|         1|          74|         243|              6|         4.99|       18.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        18.8|           2|        1|                 0.0|\n",
      "|    null| 2020-01-03 14:37:00|  2020-01-03 14:57:00|              null|      null|          61|          52|           null|         2.92|      14.38|  0.0|    0.0|       0.0|         0.0|     null|                  0.3|       14.68|        null|     null|                null|\n",
      "|       2| 2020-01-19 02:36:01|  2020-01-19 02:45:58|                 N|         5|          65|          49|              2|         1.79|       35.0|  0.0|    0.0|     10.59|         0.0|     null|                  0.3|       45.89|           1|        2|                 0.0|\n",
      "|       2| 2020-01-07 15:35:53|  2020-01-07 15:59:53|                 N|         1|          82|         162|              1|         5.07|       19.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|       22.55|           2|        1|                2.75|\n",
      "|       2| 2020-01-16 19:19:56|  2020-01-16 19:24:55|                 N|         1|          75|         236|              1|         0.97|        5.5|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|       10.05|           2|        1|                2.75|\n",
      "|    null| 2020-01-08 12:02:00|  2020-01-08 12:20:00|              null|      null|         127|          42|           null|         4.96|      21.01| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       24.06|        null|     null|                null|\n",
      "|       2| 2020-01-29 18:21:27|  2020-01-29 18:22:10|                 N|         1|          14|          14|              1|         0.32|        3.0|  1.0|    0.5|       0.0|         0.0|     null|                  0.3|         4.8|           2|        1|                 0.0|\n",
      "|       2| 2020-01-31 09:28:46|  2020-01-31 09:37:24|                 N|         1|         134|         130|              1|         1.49|        8.5|  0.0|    0.5|       2.0|         0.0|     null|                  0.3|        11.3|           1|        1|                 0.0|\n",
      "|       2| 2020-01-26 17:52:06|  2020-01-26 18:18:05|                 N|         1|         133|          22|              5|         3.94|       18.0|  0.0|    0.5|       2.0|         0.0|     null|                  0.3|       22.75|           1|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c3751a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dad5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batch(partitions):\n",
    "    return  [one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a6a1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/05 09:37:24 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 9)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/10/05 09:37:24 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 9) (marrie executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/10/05 09:37:24 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n",
      "23/10/05 09:37:24 WARN TaskSetManager: Lost task 2.0 in stage 6.0 (TID 11) (marrie executor driver): TaskKilled (Stage cancelled)\n",
      "23/10/05 09:37:24 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 10) (marrie executor driver): TaskKilled (Stage cancelled)\n",
      "23/10/05 09:37:24 WARN TaskSetManager: Lost task 3.0 in stage 6.0 (TID 12) (marrie executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 9) (marrie executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(apply_batch)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 9) (marrie executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd.mapPartitions(apply_batch).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
