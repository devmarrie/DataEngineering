{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import s3_file_operations as s3_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Character data from S3...\n",
      "An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.\n",
      "Reading Episode data from S3...\n",
      "An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.\n",
      "Reading Location data from S3...\n",
      "An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in loading data from S3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacters DataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcharacters_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes DataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocations DataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Model the proposed schema\n",
    "\n",
    "bucket = \"de-masterclass-ricknmorty\"  # S3 bucket name\n",
    "\n",
    "# Read data from S3\n",
    "print(\"Reading Character data from S3...\")\n",
    "characters_df = s3_ops.read_csv_from_s3(bucket, 'Rick&Morty/Untransformed/Character.csv')\n",
    "\n",
    "print(\"Reading Episode data from S3...\")\n",
    "episodes_df = s3_ops.read_csv_from_s3(bucket, 'Rick&Morty/Untransformed/Episode.csv')\n",
    "\n",
    "print(\"Reading Location data from S3...\")\n",
    "location_df = s3_ops.read_csv_from_s3(bucket, 'Rick&Morty/Untransformed/Location.csv')\n",
    "\n",
    "\n",
    "# Check if data is loaded successfully\n",
    "if characters_df is None or episodes_df is None or location_df is None:\n",
    "    print(\"Error in loading data from S3\")\n",
    "else:\n",
    "    print(f\"Characters DataFrame shape: {characters_df.shape}\")\n",
    "    print(f\"Episodes DataFrame shape: {episodes_df.shape}\")\n",
    "    print(f\"Locations DataFrame shape: {location_df.shape}\")\n",
    "\n",
    "\n",
    "print(\"Data loaded successfully from S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform origin_id and location_id into just ints and not json then drop the old columns\n",
    "\n",
    "def character_trans(characters_df):\n",
    "    # Function to extract the ID from a URL\n",
    "    extract_id = lambda x: x.split('/')[-1] if x else None\n",
    "\n",
    "    # Using list comprehension to extract origin_id and location_id\n",
    "    characters_df['origin_id'] = [\n",
    "        extract_id(ast.literal_eval(record)['url']) if isinstance(record, str) else None\n",
    "        for record in characters_df['origin']\n",
    "    ]\n",
    "\n",
    "    characters_df['location_id'] = [\n",
    "        extract_id(ast.literal_eval(record)['url']) if isinstance(record, str) else None\n",
    "        for record in characters_df['location']\n",
    "    ]\n",
    "    \n",
    "    # Drop and rename columns\n",
    "    print(\"Dropping and renaming columns...\")\n",
    "    characters_df = characters_df.drop(columns=['origin', 'location', 'episode'])\n",
    "\n",
    "    characters_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Appearance Dataframe Creation\n",
    "Here we will need to perform the following operations;\n",
    "- Load the Episodes df that will be our baseline table for the appearance dataframe\n",
    "- Extract character ids from the character column that consists of a list of urls belonging to all characters that appeared in that particular episode\n",
    "- Explode the resulting dataframe so that we now have each episode with its respective character as a new row.\n",
    "- Reset the index of the resulting dataframe so as to generate a new incremental column that will act as the primary key of the new column\n",
    "- Rename the new columns to resemble the proposed schema,\n",
    "    1. new_index -> ***id***\n",
    "    2. id -> ***episode_id***\n",
    "    3. character_ids -> ***character_id***\n",
    "- Once this is done we can now drop all other pre-existing columns that belonged to the episodes dataframe and only retain the three columns generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appearance Table \n",
    "def appearance(episodes_df):\n",
    "    appearance_df = episodes_df.copy()\n",
    "\n",
    "    # Function to extract the ID from a URL\n",
    "    character_func = lambda x: [url.split('/')[-1] for url in ast.literal_eval(x)] if isinstance(x, str) else None\n",
    "\n",
    "    # Using list comprehension to extract character_ids\n",
    "    appearance_df['character_ids'] = [\n",
    "        character_func(record) if record else None\n",
    "        for record in appearance_df['characters']\n",
    "    ]\n",
    "\n",
    "    # Explode the 'character_ids' column to create a row for each character ID\n",
    "    expanded_df = appearance_df.explode('character_ids')\n",
    "\n",
    "    # Reset the index to create a new 'id' column\n",
    "    expanded_df = expanded_df.reset_index(drop=True).reset_index().rename(columns={'index': 'id_new'})\n",
    "\n",
    "    # Rename columns to match the desired output\n",
    "    expanded_df = expanded_df.rename(columns={'id_new': 'id', 'id': 'episode_id', 'character_ids': 'character_id'})\n",
    "\n",
    "    # Select only the relevant columns\n",
    "    expanded_df = expanded_df[['id', 'episode_id', 'character_id']]\n",
    "\n",
    "    print(expanded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes dataframe (Only drop the character column since we can fetch it using the appearance table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
