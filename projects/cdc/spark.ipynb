{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./change/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./change/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import  SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/26 14:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 21:52:58--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 20.87.245.0\n",
      "Connecting to github.com (github.com)|20.87.245.0|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240312T185300Z&X-Amz-Expires=300&X-Amz-Signature=fd6a34724da23ff593705b0b433458c3c6be1a79d4ea732c1c334be46cd40ee2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-12 21:52:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240312T185300Z&X-Amz-Expires=300&X-Amz-Signature=fd6a34724da23ff593705b0b433458c3c6be1a79d4ea732c1c334be46cd40ee2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv.1’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-03-12 21:53:00 (9.14 MB/s) - ‘taxi_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n"
     ]
    }
   ],
   "source": [
    "!head taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading batch data is just read but stream readstream\n",
    "df = spark.read.csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|       _c0|          _c1|                 _c2|         _c3|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show it that the csv file has a header\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher volume data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 22:09:25--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 20.87.245.0\n",
      "Connecting to github.com (github.com)|20.87.245.0|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240312T190936Z&X-Amz-Expires=300&X-Amz-Signature=18550699d527c42c73f98302b465c4f2dd75b61a2c377ab8a2123c0ca388083a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-12 22:09:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240312T190936Z&X-Amz-Expires=300&X-Amz-Signature=18550699d527c42c73f98302b465c4f2dd75b61a2c377ab8a2123c0ca388083a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129967421 (124M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 123.95M   303KB/s    in 5m 17s  \n",
      "\n",
      "2024-03-12 22:14:56 (400 KB/s) - ‘fhvhv_tripdata_2021-01.csv.gz’ saved [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508066 fhvhv_tripdata_2021-01.csv.gz\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-01.csv.gz #count no of rows in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5) # first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema #to see how spark infers everything as just strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just the fist 100\n",
    "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a spark dataframe from the pandas df\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "            types.StructField('hvfhs_license_num',types.StringType(), True), \n",
    "            types.StructField('dispatching_base_num',types.StringType(), True), \n",
    "            types.StructField('pickup_datetime',types.TimestampType(), True), \n",
    "            types.StructField('dropoff_datetime',types.TimestampType(), True), \n",
    "            types.StructField('PULocationID',types.IntegerType(), True), #long 8 byte int 4 byte\n",
    "            types.StructField('DOLocationID',types.IntegerType(), True), \n",
    "            types.StructField('SR_Flag',types.StringType(), True)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 6, 59), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 43, 1), PULocationID=88, DOLocationID=42, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 50), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 4, 57), PULocationID=42, DOLocationID=151, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 14, 30), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 50, 27), PULocationID=71, DOLocationID=226, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 22, 54), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 30, 20), PULocationID=112, DOLocationID=255, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 40, 12), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 53, 31), PULocationID=255, DOLocationID=232, SR_Flag=None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid idle executors in the case of one big file. It is a lazy command\n",
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv_tripdata') #repatition executes once this runs (ls -l | wc -l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv_tripdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # we see that parquet infers the correct schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods you can perform on a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\n",
      "HV0003,B02764,2021-01-01 00:48:14,2021-01-01 01:08:42,143,78,\n",
      "HV0005,B02510,2021-01-01 00:06:59,2021-01-01 00:43:01,88,42,\n",
      "HV0005,B02510,2021-01-01 00:50:00,2021-01-01 01:04:57,42,151,\n",
      "HV0003,B02764,2021-01-01 00:14:30,2021-01-01 00:50:27,71,226,\n",
      "HV0003,B02875,2021-01-01 00:22:54,2021-01-01 00:30:20,112,255,\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|\n",
      "+-------------------+-------------------+------------+\n",
      "|2021-01-02 20:09:18|2021-01-02 20:33:50|          88|\n",
      "|2021-01-01 03:27:24|2021-01-01 03:52:36|          87|\n",
      "|2021-01-01 20:00:08|2021-01-01 20:15:49|         138|\n",
      "|2021-01-02 09:56:56|2021-01-02 10:05:42|          76|\n",
      "|2021-01-03 08:13:05|2021-01-03 08:19:01|          89|\n",
      "|2021-01-02 18:37:23|2021-01-02 19:16:02|         132|\n",
      "|2021-01-06 21:44:31|2021-01-06 21:48:51|          43|\n",
      "|2021-01-06 13:30:13|2021-01-06 13:37:38|          15|\n",
      "|2021-01-04 21:05:54|2021-01-04 21:19:09|         205|\n",
      "|2021-01-04 23:55:34|2021-01-05 00:07:21|          74|\n",
      "|2021-01-06 23:46:57|2021-01-06 23:59:49|         114|\n",
      "|2021-01-03 14:20:10|2021-01-03 14:38:15|         164|\n",
      "|2021-01-01 16:25:42|2021-01-01 16:47:24|         227|\n",
      "|2021-01-01 17:59:24|2021-01-01 18:09:30|          92|\n",
      "|2021-01-01 02:46:29|2021-01-01 02:59:51|         150|\n",
      "|2021-01-05 12:52:25|2021-01-05 13:41:08|          17|\n",
      "|2021-01-04 09:14:41|2021-01-04 09:36:35|         159|\n",
      "|2021-01-06 13:44:25|2021-01-06 14:02:41|         235|\n",
      "|2021-01-04 08:32:11|2021-01-04 08:36:15|         174|\n",
      "|2021-01-06 08:05:34|2021-01-06 08:18:58|          82|\n",
      "+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter on its own is lazy(an action but show is a transformation)\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID') \\\n",
    "   .filter(df.hvfhs_license_num == 'HV0005') \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions available in spark\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|\n",
      "+-----------+------------+------------+\n",
      "| 2021-01-02|  2021-01-02|          88|\n",
      "| 2021-01-01|  2021-01-01|          87|\n",
      "| 2021-01-01|  2021-01-01|         138|\n",
      "| 2021-01-02|  2021-01-02|          76|\n",
      "| 2021-01-03|  2021-01-03|          89|\n",
      "| 2021-01-02|  2021-01-02|         132|\n",
      "| 2021-01-06|  2021-01-06|          43|\n",
      "| 2021-01-06|  2021-01-06|          15|\n",
      "| 2021-01-04|  2021-01-04|         205|\n",
      "| 2021-01-04|  2021-01-05|          74|\n",
      "| 2021-01-06|  2021-01-06|         114|\n",
      "| 2021-01-03|  2021-01-03|         164|\n",
      "| 2021-01-01|  2021-01-01|         227|\n",
      "| 2021-01-01|  2021-01-01|          92|\n",
      "| 2021-01-01|  2021-01-01|         150|\n",
      "| 2021-01-05|  2021-01-05|          17|\n",
      "| 2021-01-04|  2021-01-04|         159|\n",
      "| 2021-01-06|  2021-01-06|         235|\n",
      "| 2021-01-04|  2021-01-04|         174|\n",
      "| 2021-01-06|  2021-01-06|          82|\n",
      "+-----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".withColumn('pickup_date', f.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', f.to_date(df.dropoff_datetime)) \\\n",
    ".select('pickup_date', 'dropoff_date', 'PULocationID') \\\n",
    ".filter(df.hvfhs_license_num == 'HV0005') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-01-06 00:34:29|2021-01-06 00:40:08|         243|         244|   NULL|\n",
      "|           HV0005|              B02510|2021-01-02 20:09:18|2021-01-02 20:33:50|          88|         112|   NULL|\n",
      "|           HV0003|              B02869|2021-01-03 17:10:54|2021-01-03 17:23:17|          20|         241|   NULL|\n",
      "|           HV0003|              B02882|2021-01-07 07:15:38|2021-01-07 07:22:37|          56|          82|   NULL|\n",
      "|           HV0003|              B02872|2021-01-06 08:09:45|2021-01-06 08:30:01|          79|         181|   NULL|\n",
      "|           HV0003|              B02872|2021-01-01 21:43:11|2021-01-01 22:12:33|         129|          19|   NULL|\n",
      "|           HV0003|              B02869|2021-01-01 09:21:19|2021-01-01 09:36:24|          90|          66|   NULL|\n",
      "|           HV0003|              B02872|2021-01-03 15:48:21|2021-01-03 15:58:35|          89|          89|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 03:49:44|2021-01-01 04:21:22|         225|         246|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 03:27:24|2021-01-01 03:52:36|          87|          61|   NULL|\n",
      "|           HV0003|              B02875|2021-01-05 14:13:03|2021-01-05 14:26:53|         134|          95|   NULL|\n",
      "|           HV0003|              B02877|2021-01-02 23:18:34|2021-01-02 23:23:51|         236|         263|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 20:00:08|2021-01-01 20:15:49|         138|         247|   NULL|\n",
      "|           HV0003|              B02765|2021-01-02 18:26:56|2021-01-02 18:30:55|          80|         256|   NULL|\n",
      "|           HV0003|              B02875|2021-01-04 13:19:12|2021-01-04 13:46:10|          76|          85|   NULL|\n",
      "|           HV0003|              B02866|2021-01-06 17:14:35|2021-01-06 17:27:33|         181|         181|   NULL|\n",
      "|           HV0003|              B02884|2021-01-06 20:25:59|2021-01-06 20:31:45|          35|          77|   NULL|\n",
      "|           HV0005|              B02510|2021-01-02 09:56:56|2021-01-02 10:05:42|          76|          76|   NULL|\n",
      "|           HV0003|              B02875|2021-01-06 14:14:19|2021-01-06 14:21:43|          37|         198|   NULL|\n",
      "|           HV0003|              B02883|2021-01-04 21:30:36|2021-01-04 21:41:18|         215|         131|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined functions example\n",
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f't/{num:03x}'\n",
    "    else:\n",
    "        return f'n/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n/b3b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02875')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_udf = f.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+------------+\n",
      "|crazy_base|pickup_date|dropoff_date|PULocationID|\n",
      "+----------+-----------+------------+------------+\n",
      "|     n/9ce| 2021-01-02|  2021-01-02|          88|\n",
      "|     n/9ce| 2021-01-01|  2021-01-01|          87|\n",
      "|     n/9ce| 2021-01-01|  2021-01-01|         138|\n",
      "|     n/9ce| 2021-01-02|  2021-01-02|          76|\n",
      "|     n/9ce| 2021-01-03|  2021-01-03|          89|\n",
      "|     n/9ce| 2021-01-02|  2021-01-02|         132|\n",
      "|     n/9ce| 2021-01-06|  2021-01-06|          43|\n",
      "|     n/9ce| 2021-01-06|  2021-01-06|          15|\n",
      "|     n/9ce| 2021-01-04|  2021-01-04|         205|\n",
      "|     n/9ce| 2021-01-04|  2021-01-05|          74|\n",
      "|     n/9ce| 2021-01-06|  2021-01-06|         114|\n",
      "|     n/9ce| 2021-01-03|  2021-01-03|         164|\n",
      "|     n/9ce| 2021-01-01|  2021-01-01|         227|\n",
      "|     n/9ce| 2021-01-01|  2021-01-01|          92|\n",
      "|     n/9ce| 2021-01-01|  2021-01-01|         150|\n",
      "|     n/9ce| 2021-01-05|  2021-01-05|          17|\n",
      "|     n/9ce| 2021-01-04|  2021-01-04|         159|\n",
      "|     n/9ce| 2021-01-06|  2021-01-06|         235|\n",
      "|     n/9ce| 2021-01-04|  2021-01-04|         174|\n",
      "|     n/9ce| 2021-01-06|  2021-01-06|          82|\n",
      "+----------+-----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".withColumn('pickup_date', f.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', f.to_date(df.dropoff_datetime)) \\\n",
    ".withColumn('crazy_base', crazy_udf(df.dispatching_base_num)) \\\n",
    ".select('crazy_base','pickup_date', 'dropoff_date', 'PULocationID') \\\n",
    ".filter(df.hvfhs_license_num == 'HV0005') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on implementation with another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-22 09:06:32--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 20.87.245.0\n",
      "Connecting to github.com (github.com)|20.87.245.0|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/f6895842-79e6-4a43-9458-e5b0b454a340?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240322%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240322T060633Z&X-Amz-Expires=300&X-Amz-Signature=8f84b634a2c06cc7af5814261bb9b2e0b62581d5d0bd52294234a7c39e2735f1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-22 09:06:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/f6895842-79e6-4a43-9458-e5b0b454a340?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240322%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240322T060633Z&X-Amz-Expires=300&X-Amz-Signature=8f84b634a2c06cc7af5814261bb9b2e0b62581d5d0bd52294234a7c39e2735f1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25031880 (24M) [application/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2021-01.csv.gz’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  23.87M   961KB/s    in 18s     \n",
      "\n",
      "2024-03-22 09:06:54 (1.31 MB/s) - ‘yellow_tripdata_2021-01.csv.gz’ saved [25031880/25031880]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"yellow_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.read_csv(\"yellow_tripdata_2021-01.csv.gz\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                   int64\n",
       "tpep_pickup_datetime      object\n",
       "tpep_dropoff_datetime     object\n",
       "passenger_count            int64\n",
       "trip_distance            float64\n",
       "RatecodeID                 int64\n",
       "store_and_fwd_flag        object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "payment_type               int64\n",
       "fare_amount              float64\n",
       "extra                    float64\n",
       "mta_tax                  float64\n",
       "tip_amount               float64\n",
       "tolls_amount             float64\n",
       "improvement_surcharge    float64\n",
       "total_amount             float64\n",
       "congestion_surcharge     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', LongType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True), \n",
    "    types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('passenger_count', types.IntegerType(), True), \n",
    "    types.StructField('trip_distance', types.DoubleType(), True), \n",
    "    types.StructField('RatecodeID', types.IntegerType(), True), \n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('payment_type', types.IntegerType(), True), \n",
    "    types.StructField('fare_amount', types.DoubleType(), True), \n",
    "    types.StructField('extra', types.DoubleType(), True), \n",
    "    types.StructField('mta_tax', types.DoubleType(), True), \n",
    "    types.StructField('tip_amount', types.DoubleType(), True), \n",
    "    types.StructField('tolls_amount', types.DoubleType(), True), \n",
    "    types.StructField('improvement_surcharge', types.DoubleType(), True), \n",
    "    types.StructField('total_amount', types.DoubleType(), True), \n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(yellow_schema) \\\n",
    "    .csv(\"yellow_tripdata_2021-01.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow \\\n",
    "    .repartition(4) \\\n",
    "    .write \\\n",
    "    .parquet(\"pq/yellow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2021\n",
    "# for i in range(0, 13):\n",
    "#     path= {where to read the data from}\n",
    "#     out = {where to take the data}\n",
    "#     read it using spark and write it to pq as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sql = spark.read.parquet('pq/yellow/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       2| 2021-01-31 14:48:53|  2021-01-31 14:54:55|              2|         1.39|         1|                 N|         249|         234|           1|        6.5|  0.0|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|\n",
      "|       2| 2021-01-14 07:40:14|  2021-01-14 07:50:53|              1|         2.32|         1|                 N|         234|          48|           2|       10.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|\n",
      "|       2| 2021-01-27 11:58:19|  2021-01-27 12:03:51|              1|         0.94|         1|                 N|         137|         162|           1|        5.5|  0.0|    0.5|       1.0|         0.0|                  0.3|         9.8|                 2.5|\n",
      "|       2| 2021-01-15 20:11:20|  2021-01-15 20:39:36|              2|         6.97|         1|                 N|          48|           7|           2|       24.0|  0.5|    0.5|       0.0|        6.12|                  0.3|       33.92|                 2.5|\n",
      "|       1| 2021-01-28 12:55:43|  2021-01-28 12:57:12|              1|          0.3|         1|                 N|         239|         239|           1|        3.5|  2.5|    0.5|       1.0|         0.0|                  0.3|         7.8|                 2.5|\n",
      "|    NULL| 2021-01-14 10:46:00|  2021-01-14 11:14:00|           NULL|         5.42|      NULL|              NULL|         181|          21|        NULL|      33.15|  0.0|    0.5|      2.75|         0.0|                  0.3|        36.7|                 0.0|\n",
      "|       1| 2021-01-09 15:38:25|  2021-01-09 15:42:41|              1|          1.0|         1|                 N|         140|         263|           1|        5.5|  2.5|    0.5|      1.75|         0.0|                  0.3|       10.55|                 2.5|\n",
      "|    NULL| 2021-01-21 18:01:20|  2021-01-21 19:01:03|           NULL|        12.16|      NULL|              NULL|         265|         185|        NULL|      46.57|  0.0|    0.5|       0.0|         0.0|                  0.3|       47.37|                 0.0|\n",
      "|       1| 2021-01-01 07:53:36|  2021-01-01 08:01:22|              1|          2.2|         1|                 N|         142|         236|           2|        9.0|  2.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       2| 2021-01-11 09:37:03|  2021-01-11 09:49:47|              1|         2.61|         1|                 N|         239|         100|           1|       11.5|  0.0|    0.5|      2.96|         0.0|                  0.3|       17.76|                 2.5|\n",
      "|       2| 2021-01-04 11:40:37|  2021-01-04 11:47:50|              1|          1.2|         1|                 N|         142|         163|           1|        7.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        10.3|                 2.5|\n",
      "|       2| 2021-01-29 19:21:41|  2021-01-29 19:28:01|              1|         1.06|         1|                 N|         140|         229|           1|        6.0|  1.0|    0.5|      1.75|         0.0|                  0.3|       12.05|                 2.5|\n",
      "|       1| 2021-01-26 14:29:35|  2021-01-26 14:39:01|              1|          1.1|         1|                 N|         237|          43|           1|        7.5|  2.5|    0.5|      2.15|         0.0|                  0.3|       12.95|                 2.5|\n",
      "|       2| 2021-01-27 15:11:39|  2021-01-27 15:31:25|              1|         4.22|         1|                 N|         140|         113|           1|       17.0|  0.0|    0.5|       2.0|         0.0|                  0.3|        22.3|                 2.5|\n",
      "|       2| 2021-01-08 08:48:10|  2021-01-08 08:57:43|              1|         1.87|         1|                 N|         125|         246|           1|        9.0|  0.0|    0.5|      3.08|         0.0|                  0.3|       15.38|                 2.5|\n",
      "|       2| 2021-01-28 18:41:21|  2021-01-28 18:46:55|              1|         0.97|         1|                 N|         164|         246|           1|        6.0|  1.0|    0.5|       2.7|         0.0|                  0.3|        13.0|                 2.5|\n",
      "|       2| 2021-01-14 10:21:30|  2021-01-14 10:43:06|              1|         4.42|         1|                 N|         236|         211|           1|       17.5|  0.0|    0.5|       2.0|         0.0|                  0.3|        22.8|                 2.5|\n",
      "|       2| 2021-01-14 11:29:14|  2021-01-14 11:52:00|              1|         5.15|         1|                 N|         141|         159|           1|       19.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        22.8|                 2.5|\n",
      "|       2| 2021-01-19 09:00:45|  2021-01-19 09:07:40|              1|         0.86|         1|                 N|          48|         230|           1|        6.5|  0.0|    0.5|      2.45|         0.0|                  0.3|       12.25|                 2.5|\n",
      "|       1| 2021-01-15 09:48:26|  2021-01-15 09:53:47|              1|          0.9|         1|                 N|         186|         161|           2|        5.5|  2.5|    0.5|       0.0|         0.0|                  0.3|         8.8|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have to make it a temp table to be able to run sql queries\n",
    "df_sql.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       2| 2021-01-31 14:48:53|  2021-01-31 14:54:55|              2|         1.39|         1|                 N|         249|         234|           1|        6.5|  0.0|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|\n",
      "|       2| 2021-01-14 07:40:14|  2021-01-14 07:50:53|              1|         2.32|         1|                 N|         234|          48|           2|       10.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|\n",
      "|       2| 2021-01-27 11:58:19|  2021-01-27 12:03:51|              1|         0.94|         1|                 N|         137|         162|           1|        5.5|  0.0|    0.5|       1.0|         0.0|                  0.3|         9.8|                 2.5|\n",
      "|       2| 2021-01-15 20:11:20|  2021-01-15 20:39:36|              2|         6.97|         1|                 N|          48|           7|           2|       24.0|  0.5|    0.5|       0.0|        6.12|                  0.3|       33.92|                 2.5|\n",
      "|       1| 2021-01-28 12:55:43|  2021-01-28 12:57:12|              1|          0.3|         1|                 N|         239|         239|           1|        3.5|  2.5|    0.5|       1.0|         0.0|                  0.3|         7.8|                 2.5|\n",
      "|    NULL| 2021-01-14 10:46:00|  2021-01-14 11:14:00|           NULL|         5.42|      NULL|              NULL|         181|          21|        NULL|      33.15|  0.0|    0.5|      2.75|         0.0|                  0.3|        36.7|                 0.0|\n",
      "|       1| 2021-01-09 15:38:25|  2021-01-09 15:42:41|              1|          1.0|         1|                 N|         140|         263|           1|        5.5|  2.5|    0.5|      1.75|         0.0|                  0.3|       10.55|                 2.5|\n",
      "|    NULL| 2021-01-21 18:01:20|  2021-01-21 19:01:03|           NULL|        12.16|      NULL|              NULL|         265|         185|        NULL|      46.57|  0.0|    0.5|       0.0|         0.0|                  0.3|       47.37|                 0.0|\n",
      "|       1| 2021-01-01 07:53:36|  2021-01-01 08:01:22|              1|          2.2|         1|                 N|         142|         236|           2|        9.0|  2.5|    0.5|       0.0|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       2| 2021-01-11 09:37:03|  2021-01-11 09:49:47|              1|         2.61|         1|                 N|         239|         100|           1|       11.5|  0.0|    0.5|      2.96|         0.0|                  0.3|       17.76|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use jvm8 and not 11, export JAVA_HOME=/path/to/java-8-installation, export PATH=$PATH:$JAVA_HOME/bin\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM trips_data LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_rev = spark.sql(\"\"\"\n",
    "SELECT PULocationID, DATE_TRUNC('hour', tpep_pickup_datetime) as hour, SUM(total_amount) as amount, COUNT(1) as records\n",
    "FROM trips_data\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------------+-------+\n",
      "|PULocationID|               hour|            amount|records|\n",
      "+------------+-------------------+------------------+-------+\n",
      "|           1|2021-01-01 10:00:00|              1.24|      4|\n",
      "|           1|2021-01-01 11:00:00|0.9299999999999999|      3|\n",
      "|           1|2021-01-01 14:00:00|             147.3|      1|\n",
      "|           1|2021-01-02 09:00:00|             101.8|      1|\n",
      "|           1|2021-01-02 13:00:00|              85.3|      1|\n",
      "|           1|2021-01-03 03:00:00|              80.8|      1|\n",
      "|           1|2021-01-03 09:00:00|            196.53|      5|\n",
      "|           1|2021-01-03 12:00:00|              80.8|      1|\n",
      "|           1|2021-01-03 17:00:00|              20.8|      1|\n",
      "|           1|2021-01-04 08:00:00|              40.3|      1|\n",
      "|           1|2021-01-04 10:00:00|            114.36|      1|\n",
      "|           1|2021-01-04 13:00:00|               4.8|      1|\n",
      "|           1|2021-01-04 17:00:00|             100.3|      1|\n",
      "|           1|2021-01-05 05:00:00|              85.3|      1|\n",
      "|           1|2021-01-05 16:00:00|              97.3|      1|\n",
      "|           1|2021-01-07 09:00:00|              65.3|      1|\n",
      "|           1|2021-01-07 12:00:00|            174.29|      1|\n",
      "|           1|2021-01-09 07:00:00|              90.3|      1|\n",
      "|           1|2021-01-09 13:00:00|             127.3|      1|\n",
      "|           1|2021-01-10 07:00:00|              80.8|      1|\n",
      "+------------+-------------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_rev.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_rev \\\n",
    "    .repartition(20) \\\n",
    "    .write \\\n",
    "    .parquet(\"pq/revenue/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark .read\\\n",
    "           .parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_yellow_rev.join(df_zones, df_yellow_rev.PULocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.drop('LocationID').write.parquet('pq/zone_rev/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "change",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
