import logging
from pathlib import Path
from airflow import DAG
from datetime import datetime, timedelta
import pyarrow.csv as pv
import pyarrow.parquet as pq
from airflow.operators.python import PythonOperator
from airflow.operators.bash  import BashOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
# from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator


def s3_source(bucket_name:str, key:str, csv_file: str) -> str:
    """
    Read data form an s3 key
    Args:
        bucket_name (str): The name of the S3 bucket.
        key (str): The key of the file to read.
    """
    hook = S3Hook('airflow_aws_s3_conn')
    file_name = hook.download_file(key=key, bucket_name=bucket_name, local_path=csv_file, preserve_file_name=True, use_autogenerated_subdir=False)
    return file_name

def transform_main(csv_path:str, pq_path:str):
    """Change the data type to parquet"""
    table = pv.read_csv(csv_path)
    pq.write_table(table, pq_path)
    return pq_path


def load_to_s3(pq_path: str, key: str, bucket_name:str) -> None:
    """Load the data to our s3 bucket"""
    hook = S3Hook('airflow_aws_s3_conn')
    hook.load_file(filename=pq_path, bucket_name=bucket_name, key=key)

default_args = {
    'owner': 'your_name',
    'start_date': datetime(2023, 11, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'fact_table',
    default_args=default_args,
    description='creating the fact table',
    schedule_interval='@daily',
    catchup=False,
    max_active_runs=1,
    tags=['fact']
)


months = [1,2,3,4,5,6,7,8,9,10,11,12]
year = 2020


for month in months:
    data_file = f'yellow_tripdata_{year}-{month:02}.csv.gz'
    bucket_name = 'nytaxi-data-raw-us-east-airflow-dev'
    key= f'data/yellow/{data_file}'
    csv_file= '/home/devmarrie/airflow/data/csv_to_pq/'
    csv_path= f'{csv_file}{data_file}'
    pq_file = data_file.replace('.csv', '.pq')
    pq_path= f'/home/devmarrie/airflow/data/pq/{pq_file}'
    clean_key= f'data/Trip_Fact/{pq_file}'
    clean_bucket= 'nytaxi-data-raw-us-east-airflow-dev-clensed'


    task_s3_source = PythonOperator(
        task_id=f'read_s3_source_{month}',
        python_callable=s3_source,
        op_kwargs= {
            'bucket_name': bucket_name,
            'key': key,
            'csv_file': csv_file
        },
        dag=dag
    )

    task_to_pq = PythonOperator(
        task_id=f'convert_csv_to_pq_{month}',
        python_callable=transform_main,
        op_args=[csv_path, pq_path],
        dag=dag
    )

    task_upload_to_clean = PythonOperator(
        task_id=f'upload_to_clean_{month}',
        python_callable=load_to_s3,
        op_kwargs= {
            'pq_path': pq_path,
            'key': clean_key,
            'bucket_name': clean_bucket
        },
        dag=dag
    )

    task_s3_source >> task_to_pq >> task_upload_to_clean
