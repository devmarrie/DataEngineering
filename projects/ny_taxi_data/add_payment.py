from datetime import datetime, timedelta
import pyarrow.parquet as pq
import pyarrow as pa
import io
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

def s3_source(bucket_name:str, key:str, dest: str) -> str:
    """
    Read data form an s3 key
    Args:
        bucket_name (str): The name of the S3 bucket.
        key (str): The key of the file to read.
    """
    hook = S3Hook('airflow_aws_s3_conn')
    file_name = hook.download_file(key=key, bucket_name=bucket_name, local_path=dest, preserve_file_name=True, use_autogenerated_subdir=False)
    return file_name

def transform(ti, sql_path: str) -> str:
    """Read the data and store it in a pyarrow table"""
    download_file_name = ti.xcom_pull(task_ids='retrieve_from_src')
    table = pq.read_table(download_file_name)

    if table['payment_type'] == 1.0:
        table = table.set_column('payment_mthd', pa.array(['credit card']))
    elif table['payment_type'] == 2.0:
        table = table.set_column('payment_mthd', pa.array(['cash']))
    elif table['payment_type'] == 3.0:
        table = table.set_column('payment_mthd', pa.array(['no charge']))
    elif table['payment_type'] == 4.0:
        table = table.set_column('payment_mthd', pa.array(['dispute']))
    elif table['payment_type'] == 5.0:
        table = table.set_column('payment_mthd', pa.array(['unknown']))
    elif table['payment_type'] == 6.0:
        table = table.set_column('payment_mthd', pa.array(['voided trip']))
    pq.write_table(table, sql_path)
    return sql_path
    

default_args = {
    'owner': 'marrie',
    'start_date': datetime(2023, 11, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'payment_columns',
    default_args=default_args,
    description='adding new columns to the data and pushing it to clean s3',
    schedule='@daily',
    catchup=False,
    max_active_runs=1,
    tags=['to_clean']
)

month = 1
dest = f'/home/devmarrie/airflow/data/cl/'
bucket_name = 'nytaxi-data-raw-us-east-airflow-dev'
key = 'data/yellow/yellow_tripdata_2020-01.parquet.gz'
sql_path = '/home/devmarrie/airflow/data/sql/yellow_tripdata_2020-01.parquet.gz'

task_from_source = PythonOperator(
    task_id='retrieve_from_src',
    python_callable=s3_source,
    op_kwargs= {
        'bucket_name': bucket_name,
        'key': key,
        'dest': dest
    },
    dag=dag
)


task_to_pa_table = PythonOperator(
    task_id='to_a_table',
    python_callable=transform,
    op_kwargs= {
        'sql_path': sql_path
    },
    dag=dag
)



